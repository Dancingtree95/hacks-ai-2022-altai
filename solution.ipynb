{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data\\\\train_dataset_train.csv')\n",
    "test = pd.read_csv('Data\\\\test_dataset_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import preprocess\n",
    "preprocess(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_generation import *\n",
    "\n",
    "train, test = generate_place_of_study_nb(train, test)\n",
    "train, test = generate_dev_from_group_average_birth(train, test)\n",
    "train, test = generate_age(train, test)\n",
    "train, test = generate_gap_year_dur(train, test)\n",
    "train, test = generate_is_school_certificate(train, test)\n",
    "train, test = generate_relative_rating(train, test, 'КодФакультета', 'СрБаллАттестата_отн_факультета', True, 0)\n",
    "train, test = generate_relative_rating(train, test, 'Код_группы', 'СрБаллАттестата_отн_группы', False, 1e-4)\n",
    "train, test = generate_group_freq(train, test)\n",
    "train, test = generate_in_year_diff(train, test)\n",
    "train.drop(columns = 'СрБаллАттестата', inplace = True)\n",
    "test.drop(columns = 'СрБаллАттестата', inplace = True)\n",
    "train['Код_группы_кат'] = train['Код_группы'].apply(lambda x : str(x))\n",
    "test['Код_группы_кат'] = test['Код_группы'].apply(lambda x : str(x))\n",
    "train['Код_факультета_кат'] = train['КодФакультета'].apply(lambda x : str(x))\n",
    "test['Код_факультета_кат'] = test['КодФакультета'].apply(lambda x : str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = train.drop(columns = ['ID', 'Статус'] + drop_columns), train['Статус']\n",
    "cat_cols = [col for col in X.columns if X[col].dtype == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_config = {\n",
    "                   'iterations' : 1500,\n",
    "                   'eval_metric': 'TotalF1:average=Macro',\n",
    "                   'use_best_model':True,\n",
    "                   'learning_rate': 0.08,\n",
    "                   'rsm': 0.5,\n",
    "                   'task_type' : 'CPU', \n",
    "                   'max_ctr_complexity' : 0, \n",
    "                   'depth' : 6,\n",
    "                   'random_seed': 14121995\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_utils import cross_val_catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "n_splits = 10\n",
    "average = 'macro'\n",
    "res = cross_val_catboost(catboost_config, X, Y, cat_cols,None, n_splits,  average = average, verbose = False)\n",
    "train_scores = res['train']\n",
    "test_scores = res['test']\n",
    "models = res['models']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 на обучающей выборке: 0.909 +- 0.023\n",
      "Macro F1 на тестовой выборке: 0.801 +- 0.020\n"
     ]
    }
   ],
   "source": [
    "if average == 'macro':\n",
    "    d = np.sqrt(n_splits)\n",
    "    print(f'Macro F1 на обучающей выборке: {np.mean(train_scores):.3f} +- {3 * np.std(train_scores) / d:.3f}')\n",
    "    print(f'Macro F1 на тестовой выборке: {np.mean(test_scores):.3f} +- {3 * np.std(test_scores) / d:.3f}')\n",
    "elif average == None:\n",
    "    print('F1 train score per class: %.3f/%.3f/%.3f' % tuple(np.array(train_scores).mean(axis = 0)), \n",
    "      'total: %.3f' % np.array(train_scores).mean())\n",
    "    print('F1 test score per class: %.3f/%.3f/%.3f' % tuple(np.array(test_scores).mean(axis = 0)),\n",
    "      'total: %.3f' % np.array(test_scores).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on test data and submition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for model in models:\n",
    "    preds.append(model.predict(Pool(test.drop(columns = 'ID'), cat_features = cat_cols)))\n",
    "preds = np.hstack(preds)\n",
    "test_status = np.zeros(preds.shape[0])\n",
    "for i, votes in enumerate(preds):\n",
    "    labels, counts = np.unique(votes, return_counts = True)\n",
    "    most_voted = labels[np.argmax(counts)]\n",
    "    test_status[i] = most_voted\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = pd.DataFrame({'ID' : test['ID'], 'Статус' : test_status})\n",
    "subm.to_csv('submition.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f492cb1abf92620539c60030079aacb93e34dff8b698949905a09c2556ac080"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
